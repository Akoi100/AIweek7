# AI Ethics Assignment

**Theme**: "Designing Responsible and Fair AI Systems" üåç‚öñÔ∏è

This repository contains comprehensive analysis of AI ethics principles, case studies, and a practical bias audit of the COMPAS recidivism prediction system.

---

## üìÅ Repository Structure

```
AI_Ethics_Assignment/
‚îÇ
‚îú‚îÄ‚îÄ ethics_assignment_report.md      # Parts 1, 2, 4: Theory, case studies, reflection
‚îú‚îÄ‚îÄ healthcare_ai_ethics_policy.md   # Bonus: Healthcare AI ethics guidelines
‚îÇ
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ compas_bias_audit.ipynb      # Part 3: COMPAS dataset bias analysis
‚îÇ   ‚îî‚îÄ‚îÄ bias_audit_report.md         # 300-word findings summary
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ (COMPAS dataset - download separately)
‚îÇ
‚îú‚îÄ‚îÄ README.md                         # This file
‚îî‚îÄ‚îÄ requirements.txt                  # Python dependencies
```

---

## üéØ Assignment Overview

### Part 1: Theoretical Understanding (30%)

**Q1: Algorithmic Bias**
- Definition and manifestation mechanisms
- Examples: Amazon hiring tool (gender bias), COMPAS (racial bias)

**Q2: Transparency vs. Explainability**
- Transparency: System-level openness (what the system is)
- Explainability: Instance-level interpretability (why specific decisions)
- Both necessary for trustworthy AI

**Q3: GDPR Impact on AI**
- Right to explanation (Article 22)
- Data minimization and purpose limitation
- Privacy by design requirements
- Enforcement: Up to ‚Ç¨20M or 4% global revenue

**Ethical Principles Matching**:
- Justice ‚Üí Fair distribution of AI benefits/risks
- Non-maleficence ‚Üí Do no harm
- Autonomy ‚Üí User control over data/decisions
- Sustainability ‚Üí Environmental responsibility

---

### Part 2: Case Study Analysis (40%)

#### Case 1: Amazon's Biased Hiring Tool

**Source of Bias**:
- Historical bias in training data (70% male resumes)
- Proxy features (women's colleges, gendered language)
- Feedback loop amplification

**Proposed Fixes**:
1. Reweighed training data for gender balance
2. Remove gendered features and proxies
3. Fairness-constrained optimization (equal opportunity)

**Fairness Metrics**:
- Disparate Impact Ratio (target: >0.8)
- Equal Opportunity Difference (target: <0.05)
- Average Odds Difference
- Calibration across groups

#### Case 2: Facial Recognition in Policing

**Ethical Risks**:
- Wrongful arrests (43x higher error rate for Black women)
- Privacy violations and mass surveillance
- Reinforcement of systemic racism
- Lack of accountability

**Policy Recommendations**:
1. Moratorium on real-time FRT until accuracy parity
2. Mandatory accuracy audits (>99% for all demographics)
3. Judicial oversight and warrant requirements
4. Human review and corroboration
5. Community consent and democratic oversight
6. Data protection and retention limits
7. Vendor accountability

---

### Part 3: Practical Audit (25%)

**COMPAS Recidivism Dataset Analysis**

**Key Findings**:
- **Disparate Impact**: 0.65 (violates 80% rule)
- **False Positive Rate**: Black defendants 44.9% vs. White 23.5% (2x disparity)
- **False Negative Rate**: White defendants 47.7% vs. Black 28.0%

**Visualizations**:
- Risk score distributions by race
- Confusion matrices showing error disparities
- Fairness metrics dashboard

**Remediation**:
- Suspend use until bias eliminated
- Retrain with fairness constraints
- Remove proxy features
- Continuous monitoring

---

### Part 4: Ethical Reflection (5%)

**Personal Project**: Mental health chatbot for university students

**Ethical Principles Applied**:
- **Non-maleficence**: Crisis detection, escalation to professionals
- **Autonomy**: Informed consent, data control, opt-out rights
- **Justice**: Diverse training data, cultural sensitivity, accessibility
- **Transparency**: Clear AI disclosure, explainable recommendations
- **Privacy**: End-to-end encryption, anonymization, HIPAA compliance

---

### Bonus Task (10%)

**Healthcare AI Ethics Policy**

**Key Components**:
1. **Patient Consent**: Informed consent forms, opt-out rights, vulnerable population protections
2. **Bias Mitigation**: Pre-deployment audits, diverse training data, continuous monitoring
3. **Transparency**: Model cards, explainable AI, audit trails
4. **Accountability**: Human oversight, clinical validation board, liability framework
5. **Privacy**: HIPAA compliance, de-identification, data minimization

---

## üöÄ How to Run the COMPAS Bias Audit

### Prerequisites

```bash
# Install Python 3.8+
python --version

# Install dependencies
pip install -r requirements.txt
```

### Download COMPAS Dataset

```bash
# Option 1: From ProPublica GitHub
wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv -O data/compas-scores-two-years.csv

# Option 2: Manual download
# Visit: https://github.com/propublica/compas-analysis
# Download compas-scores-two-years.csv to data/ folder
```

### Run the Audit

```bash
# Launch Jupyter Notebook
jupyter notebook code/compas_bias_audit.ipynb

# Or run as Python script (if converted)
python code/compas_bias_audit.py
```

**Expected Output**:
- Fairness metrics (disparate impact, equal opportunity)
- Confusion matrices by race
- Visualizations of bias
- 300-word findings report

---

## üìä Key Results

### COMPAS Bias Metrics

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Disparate Impact Ratio** | 0.65 | Violates 80% rule (adverse impact) |
| **FPR (Black)** | 44.9% | Nearly 2x higher than white defendants |
| **FPR (White)** | 23.5% | Significantly lower false positive rate |
| **Equal Opportunity Diff** | +0.197 | Unequal treatment of recidivists |

**Conclusion**: COMPAS exhibits systematic racial bias and should not be used for consequential decisions.

---

## üìö Technologies Used

- **Python 3.8+**
- **AI Fairness 360**: IBM's bias detection and mitigation toolkit
- **Pandas & NumPy**: Data manipulation
- **Matplotlib & Seaborn**: Visualization
- **Jupyter Notebook**: Interactive analysis

---

## üåê Real-World Impact

This assignment demonstrates:

‚úÖ **Critical Analysis**: Understanding sources and manifestations of algorithmic bias  
‚úÖ **Practical Skills**: Using AI Fairness 360 to audit real-world systems  
‚úÖ **Policy Thinking**: Proposing actionable solutions to ethical dilemmas  
‚úÖ **Stakeholder Awareness**: Considering impacts on vulnerable populations  
‚úÖ **Regulatory Knowledge**: GDPR, healthcare regulations, legal standards  

---

## üìñ Learning Outcomes

- ‚úÖ Define and identify algorithmic bias
- ‚úÖ Distinguish transparency from explainability
- ‚úÖ Understand GDPR's impact on AI development
- ‚úÖ Analyze case studies (Amazon hiring, facial recognition)
- ‚úÖ Conduct bias audits using AI Fairness 360
- ‚úÖ Propose fairness metrics and remediation strategies
- ‚úÖ Draft ethical AI policies for healthcare
- ‚úÖ Reflect on personal project ethics

---

## üìÑ References

1. Angwin, J., et al. (2016). Machine Bias. *ProPublica*.
2. Buolamwini, J., & Gebru, T. (2018). Gender Shades. *Proceedings of Machine Learning Research*.
3. Dastin, J. (2018). Amazon scraps secret AI recruiting tool. *Reuters*.
4. European Commission. (2019). Ethics Guidelines for Trustworthy AI.
5. Bellamy, R. K., et al. (2019). AI Fairness 360. *IBM Journal of Research and Development*.

---

## üë• Contributors

[Peer Group Members]

---

## üìÑ License

This project is submitted as part of the AI Ethics course assignment.

---

**Submission**: GitHub Repository - https://github.com/Akoi100/AIweek7  
**Date**: November 28, 2025  
**Course**: AI Ethics
